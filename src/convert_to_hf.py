import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# ================= é…ç½®åŒº =================
# 1. ä½ çš„ SFT åŸºåº§æ¨¡å‹è·¯å¾„ (ç”¨äºåˆå§‹åŒ–ç»“æ„)
BASE_MODEL_PATH = "/data/private/ExeCoder/results/Deepseek-coder-6.7b-instruct-code/checkpoint-327"

# 2. ä½ çš„ DeepSpeed Checkpoint è·¯å¾„
# æ³¨æ„ï¼šæŒ‡å‘ checkpoint-500 ç›®å½•å³å¯
CHECKPOINT_PATH = "/data/private/ExeCoder/results/dpo_online_mask_v1/checkpoint-500"

# 3. æœ€ç»ˆè¾“å‡ºè·¯å¾„
OUTPUT_PATH = "/data/private/ExeCoder/results/dpo_online_mask_v1/checkpoint-500-hf"
# =========================================

def main():
    print(f"ğŸš€ å¼€å§‹è½¬æ¢ ZeRO-2 Checkpoint...")
    print(f"ğŸ“‚ Checkpoint è·¯å¾„: {CHECKPOINT_PATH}")

    # 1. å¯»æ‰¾ Rank 0 çš„æƒé‡æ–‡ä»¶
    # DeepSpeed ZeRO-2 é€šå¸¸å‘½åä¸º mp_rank_00_model_states.pt
    target_file = os.path.join(CHECKPOINT_PATH, "mp_rank_00_model_states.pt")
    
    if not os.path.exists(target_file):
        # å¤‡é€‰æ–¹æ¡ˆï¼šæœ‰æ—¶å€™å¯èƒ½æ˜¯ global_stepXXX/mp_rank_00...
        print(f"âš ï¸ æœªç›´æ¥æ‰¾åˆ° {target_file}ï¼Œå°è¯•æœç´¢å­ç›®å½•...")
        found = False
        for root, dirs, files in os.walk(CHECKPOINT_PATH):
            if "mp_rank_00_model_states.pt" in files:
                target_file = os.path.join(root, "mp_rank_00_model_states.pt")
                found = True
                break
        if not found:
            raise FileNotFoundError(f"âŒ æ— æ³•åœ¨ {CHECKPOINT_PATH} ä¸­æ‰¾åˆ° mp_rank_00_model_states.pt æ–‡ä»¶ï¼è¯·æ£€æŸ¥ç›®å½•æ˜¯å¦ä¸ºç©ºã€‚")

    print(f"âœ… æ‰¾åˆ°æƒé‡æ–‡ä»¶: {target_file}")

    # 2. åŠ è½½åŸºåº§æ¨¡å‹ç»“æ„ (åŠ è½½åˆ° CPU å†…å­˜ï¼Œé¿å…çˆ†æ˜¾å­˜)
    print("â³ æ­£åœ¨åˆå§‹åŒ–åŸºåº§æ¨¡å‹ç»“æ„...")
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_PATH,
        torch_dtype=torch.bfloat16, # ä¿æŒå’Œè®­ç»ƒæ—¶ä¸€è‡´
        trust_remote_code=True,
        device_map="cpu" # å¼ºåˆ¶ CPU
    )
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)

    # 3. åŠ è½½ DeepSpeed æƒé‡
    print(f"â³ æ­£åœ¨åŠ è½½ DeepSpeed æƒé‡ (è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...")
    # map_location='cpu' å…³é”®ï¼Œé˜²æ­¢å ç”¨ GPU
    state_dict = torch.load(target_file, map_location='cpu')

    # DeepSpeed ä¿å­˜çš„ state_dict é€šå¸¸åŒ…è£¹åœ¨ 'module' é”®ä¸‹
    if "module" in state_dict:
        print("â„¹ï¸ æ£€æµ‹åˆ° 'module' å‰ç¼€ï¼Œæ­£åœ¨å‰¥ç¦»...")
        state_dict = state_dict["module"]
    
    # æœ‰æ—¶å€™ key ä¼šå¸¦æœ‰ 'module.' å‰ç¼€ (DDP é—ç•™)ï¼Œéœ€è¦å»é™¤
    new_state_dict = {}
    for k, v in state_dict.items():
        if k.startswith("module."):
            new_state_dict[k[7:]] = v
        else:
            new_state_dict[k] = v
            
    # 4. è¦†ç›–æƒé‡
    print("â³ æ­£åœ¨å°†æƒé‡åº”ç”¨åˆ°æ¨¡å‹...")
    missing, unexpected = model.load_state_dict(new_state_dict, strict=False)
    print(f"ğŸ“„ æƒé‡åŠ è½½æŠ¥å‘Š: Missing keys: {len(missing)}, Unexpected keys: {len(unexpected)}")
    
    if len(missing) > 0:
        print(f"âš ï¸ è­¦å‘Š: ä¸¢å¤±äº† {len(missing)} ä¸ªé”® (å¯èƒ½æ˜¯ LoRA æˆ–éå…³é”®å‚æ•°ï¼Œå¦‚æœæ•°é‡å¾ˆå¤§è¯·è­¦æƒ•)")
    
    # 5. ä¿å­˜ä¸º HF æ ¼å¼
    print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜ä¸º Safetensors æ ¼å¼åˆ°: {OUTPUT_PATH}")
    model.save_pretrained(OUTPUT_PATH, safe_serialization=True, max_shard_size="10GB")
    tokenizer.save_pretrained(OUTPUT_PATH)
    
    print("ğŸ‰ è½¬æ¢å®Œæˆï¼ç°åœ¨å¯ä»¥ä½¿ç”¨è¯¥æ¨¡å‹è¿›è¡Œæ¨ç†äº†ã€‚")

if __name__ == "__main__":
    main()