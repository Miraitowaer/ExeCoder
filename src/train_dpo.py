import os
from dataclasses import dataclass, field
from typing import Dict, Optional

import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    HfArgumentParser,
)
from trl import DPOTrainer, DPOConfig

# ================= é…ç½®å‚æ•° =================
@dataclass
class ScriptArguments:
    model_name_or_path: str = field(metadata={"help": "Path to the SFT model"})
    data_path: str = field(metadata={"help": "Path to the DPO dataset (json)"})
    ignore_index: int = field(default=-100, metadata={"help": "Label value to ignore for loss"})

# ================= æ ¸å¿ƒï¼šæ¨¡æ¿å¯¹é½å‡½æ•° =================
def apply_deepseek_template(example):
    """
    [å…³é”®ä¿®æ”¹]
    å°†åŸå§‹ DPO æ•°æ®é›†ä¸­çš„ Prompt åŒ…è£…æˆä¸ SFT å®Œå…¨ä¸€è‡´çš„ Alpaca æ ¼å¼ï¼ˆåŒ…å« Preambleï¼‰ã€‚
    """
    prompt_raw = example["prompt"]
    
    # SFT ä¸­ä½¿ç”¨çš„ Preamble (å¼€åœºç™½)
    # å¿…é¡»å’Œä½  train.py ä¸­ç”Ÿæ•ˆçš„é‚£ä¸ªç‰ˆæœ¬ä¸€æ¨¡ä¸€æ ·ï¼Œä¸€ä¸ªæ ‡ç‚¹éƒ½ä¸èƒ½å·®ï¼
    ALPACA_PREAMBLE = (
        "Below is an instruction that describes a task, paired with an input that provides further context. "
        "Write a response that appropriately completes the request.\n\n"
    )
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»åŒ…å«äº†æ¨¡æ¿ (é˜²æ­¢é‡å¤æ·»åŠ )
    if "### Response:" not in prompt_raw:
        # æ„é€ æ ‡å‡†çš„ Alpaca æ ¼å¼ï¼šPreamble + Instruction + Response
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬å°† prompt_raw (åŒ…å«æŒ‡ä»¤å’Œä»£ç ) æ•´ä½“æ”¾å…¥ ### Instruction: å—ä¸­
        # è¿™æ˜¯å› ä¸ºæˆ‘ä»¬çš„ mining è„šæœ¬æ²¡æœ‰åŒºåˆ† instruction å’Œ input å­—æ®µï¼Œè¿™æ˜¯ç›®å‰å…¼å®¹æ€§æœ€å¥½çš„åšæ³•
        new_prompt = f"{ALPACA_PREAMBLE}### Instruction:\n{prompt_raw}\n\n### Response:\n"
    else:
        new_prompt = prompt_raw
        
    example["prompt"] = new_prompt
    return example

def main():
    # 1. è§£æå‚æ•°
    parser = HfArgumentParser((ScriptArguments, DPOConfig))
    script_args, training_args = parser.parse_args_into_dataclasses()

    # 2. åŠ è½½ Policy Model (SFTåçš„æ¨¡å‹)
    print(f"Loading Policy Model from {script_args.model_name_or_path}...")
    model = AutoModelForCausalLM.from_pretrained(
        script_args.model_name_or_path,
        trust_remote_code=True,
        use_cache=False,
    )
    
    # 3. åŠ è½½ Reference Model (å‚è€ƒæ¨¡å‹)
    # åœ¨ DeepSpeed ZeRO-3 æ¨¡å¼ä¸‹ï¼Œæ˜¾å¼åŠ è½½é€šå¸¸æ›´ç¨³å¥
    print(f"Loading Reference Model from {script_args.model_name_or_path}...")
    ref_model = AutoModelForCausalLM.from_pretrained(
        script_args.model_name_or_path,
        trust_remote_code=True,
        use_cache=False,
    )

    # 4. åŠ è½½ Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(script_args.model_name_or_path, trust_remote_code=True)
    if tokenizer.pad_token is None:
        if tokenizer.eos_token is not None:
            tokenizer.pad_token = tokenizer.eos_token
        else:
            tokenizer.add_special_tokens({'pad_token': '<|endoftext|>'})
            tokenizer.pad_token_id = 151643 # Qwen çš„é»˜è®¤ pad id
            
    # [é‡è¦] Qwen2.5 DPO å»ºè®®ï¼šæ˜ç¡®æ‰“å°ä¸€ä¸‹ ID ç¡®ä¿åŠ è½½æ­£ç¡®
    print(f"ğŸ”¥ Tokenizer Loaded | EOS: {tokenizer.eos_token_id} | PAD: {tokenizer.pad_token_id}")
    
    model.config.pad_token_id = tokenizer.pad_token_id
    
    if ref_model is not None:
        ref_model.config.pad_token_id = tokenizer.pad_token_id
        
    if model.generation_config is not None:
        model.generation_config.pad_token_id = tokenizer.pad_token_id
        
    print(f"ğŸ”§ Model Config Updated: model.config.pad_token_id = {model.config.pad_token_id}")
        
    # 5. åŠ è½½æ•°æ®
    print(f"Loading dataset from {script_args.data_path}...")
    dataset = load_dataset('json', data_files=script_args.data_path, split='train')

    # ================== æ ¸å¿ƒä¿®æ”¹åŒºåŸŸ ==================
    print("Applying DeepSeek-Coder (Alpaca) template to prompts...")
    # å¯¹æ•°æ®é›†çš„æ¯ä¸€è¡Œåº”ç”¨æ¨¡æ¿è½¬æ¢
    dataset = dataset.map(apply_deepseek_template)
    
    # æ‰“å°ä¸€æ¡æ ·æœ¬è¿›è¡Œäººå·¥æ ¸å¯¹ (Sanity Check)
    # è¯·åœ¨æ—¥å¿—ä¸­æ£€æŸ¥æ˜¯å¦å‡ºç°äº† "### Instruction:"
    print("="*40)
    print(f"Sample Prompt Preview (Aligned with SFT):\n{dataset[0]['prompt'][:200]}...")
    print("="*40)
    # =================================================

    # 6. åˆå§‹åŒ– DPOTrainer
    # ä½¿ç”¨ trl åº“çš„æ ‡å‡† DPOTrainer
    trainer = DPOTrainer(
        model=model,
        ref_model=ref_model,
        args=training_args,
        train_dataset=dataset,
        tokenizer=tokenizer,
        max_length=training_args.max_length,
        max_prompt_length=training_args.max_prompt_length,
    )

    # 7. å¼€å§‹è®­ç»ƒ
    print("Starting Standard DPO Baseline training (Template-Aligned)...")
    trainer.train()
    
    # 8. ä¿å­˜æ¨¡å‹
    print(f"Saving model to {training_args.output_dir}")
    trainer.save_model(training_args.output_dir)
    # åŒæ—¶ä¿å­˜ tokenizerï¼Œæ–¹ä¾¿åç»­ç›´æ¥åŠ è½½æ¨ç†
    tokenizer.save_pretrained(training_args.output_dir)

if __name__ == "__main__":
    main()