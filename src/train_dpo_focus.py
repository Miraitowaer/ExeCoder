import os
import torch
from dataclasses import dataclass, field
from typing import Dict, Optional, List, Tuple, Union, Any

from datasets import load_dataset, Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    HfArgumentParser,
    TrainingArguments,
)
from transformers.data.data_collator import DataCollatorMixin
from trl import DPOTrainer, DPOConfig

# =============================================================================
# 1. è‡ªå®šä¹‰ Data Collator (å¤„ç†æ–°å¢çš„ Mask å­—æ®µ)
# =============================================================================
@dataclass
class FocusedDPODataCollatorWithPadding(DataCollatorMixin):
    tokenizer: AutoTokenizer
    # æˆ‘ä»¬éœ€è¦ç‰¹æ®Šå¤„ç† extra_rejected_mask çš„ padding
    
    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:
        # 1. æå–è‡ªå®šä¹‰ maskï¼Œé˜²æ­¢è¢«é»˜è®¤ collator ä¸¢å¼ƒæˆ–æŠ¥é”™
        rejected_masks = [f.pop("extra_rejected_mask") for f in features]
        
        # 2. è°ƒç”¨é»˜è®¤çš„ padding é€»è¾‘å¤„ç† input_ids ç­‰
        batch = self.tokenizer.pad(
            features,
            padding=True,
            return_tensors="pt"
        )
        
        # 3. æ‰‹åŠ¨å¯¹ extra_rejected_mask è¿›è¡Œ padding
        # å®ƒçš„é•¿åº¦åº”è¯¥å’Œ rejected_input_ids ä¸€è‡´
        max_len = batch["rejected_input_ids"].shape[1]
        padded_masks = []
        for mask in rejected_masks:
            # æˆªæ–­
            mask = mask[:max_len]
            # å¡«å…… (ç”¨ 0.0 å¡«å……ï¼Œè¡¨ç¤º padding éƒ¨åˆ†ä¸è®¡ç®—æƒé‡)
            padded_mask = mask + [0.0] * (max_len - len(mask))
            padded_masks.append(padded_mask)
            
        batch["extra_rejected_mask"] = torch.tensor(padded_masks, dtype=torch.float32)
        return batch

# =============================================================================
# 2. è‡ªå®šä¹‰ Trainer (æ ¸å¿ƒåˆ›æ–°ç‚¹: é‡å†™ Logps è®¡ç®—)
# =============================================================================
class FocusedDPOTrainer(DPOTrainer):
    def _get_batch_logps(
        self,
        logits: torch.FloatTensor,
        labels: torch.LongTensor,
        average_log_prob: bool = False,
        is_encoder_decoder: bool = False,
        label_pad_token_id: int = -100,
        is_rejected: bool = False, # æˆ‘ä»¬ä¿®æ”¹æºç é€»è¾‘ï¼Œå¢åŠ è¿™ä¸ªæ ‡è®°åˆ¤æ–­
        focused_mask: torch.FloatTensor = None, # æ¥æ”¶æˆ‘ä»¬çš„è‡ªå®šä¹‰ mask
    ) -> Tuple[torch.FloatTensor, torch.LongTensor]:
        
        # è°ƒç”¨çˆ¶ç±»é€»è¾‘è®¡ç®—æ ‡å‡†çš„ token-level logps
        # æ³¨æ„ï¼šä¸ºäº†ä¸ç ´åçˆ¶ç±»ç­¾åï¼Œé€šå¸¸æˆ‘ä»¬éœ€è¦ trick ä¸€ä¸‹æˆ–è€…é‡å†™ forward
        # ä½† trl çš„ç»“æ„æ¯”è¾ƒç´§è€¦åˆã€‚è¿™é‡Œæˆ‘ä»¬é‡‡ç”¨æ›´åº•å±‚çš„é‡å†™æ–¹å¼ã€‚
        
        # æ ‡å‡† Logits å¤„ç†
        if logits.shape[:-1] != labels.shape:
            raise ValueError("Logits and labels must have the same shape.")

        labels = labels.clone()
        loss_mask = labels != label_pad_token_id

        # dummy token; we'll ignore the losses on these tokens later
        labels[labels == label_pad_token_id] = 0

        per_token_logps = torch.gather(logits.log_softmax(-1), dim=2, index=labels.unsqueeze(2)).squeeze(2)

        # ==========================================
        # ğŸ”¥ INNOVATION: Focused Weighting Applied Here
        # ==========================================
        if is_rejected and focused_mask is not None:
            # focused_mask: [batch, seq_len]
            # 1.0 = Error Token (Full Penalty)
            # 0.1 = Correct Token (Reduced Penalty)
            
            # ç¡®ä¿ mask å’Œ logps åœ¨åŒä¸€è®¾å¤‡
            focused_mask = focused_mask.to(per_token_logps.device)
            
            # åº”ç”¨åŠ æƒï¼š
            # æˆ‘ä»¬å¸Œæœ› Error Token çš„ logp è´¡çŒ®ä¿æŒåŸæ · (weight 1.0)
            # é Error Token çš„ logp è´¡çŒ®å˜å° (weight 0.1) -> å¯¹ loss è´¡çŒ®å˜å° -> æ¢¯åº¦å˜å°
            # ä¹Ÿå°±æ˜¯è®©æ¨¡å‹ "ä¸»è¦å»ä¼˜åŒ– Error Token çš„æ¦‚ç‡"
            per_token_logps = per_token_logps * focused_mask

        if average_log_prob:
            return (per_token_logps * loss_mask).sum(-1) / loss_mask.sum(-1), loss_mask
        else:
            return (per_token_logps * loss_mask).sum(-1), loss_mask

    # é‡å†™ concatenated_forward ä»¥ä¾¿ä¼ å…¥ is_rejected å’Œ focused_mask
    def concatenated_forward(
        self, model, batch: Dict[str, Union[List, torch.LongTensor]]
    ) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:
        
        # 1. æ„å»ºå„ç§ Input
        len_chosen = batch["chosen_labels"].shape[0]
        
        # æ‹¼æ¥ batch (Chosen + Rejected)
        concatenated_batch = self.concatenated_inputs(
            batch,
            is_encoder_decoder=self.is_encoder_decoder,
            label_pad_token_id=self.label_pad_token_id,
            padding_value=self.padding_value,
            device=self.accelerator.device,
        )
        
        # 2. æ¨¡å‹å‰å‘ä¼ æ’­
        all_logits = model(
            input_ids=concatenated_batch["concatenated_input_ids"],
            attention_mask=concatenated_batch.get("concatenated_attention_mask", None),
            use_cache=False,
        ).logits

        # 3. åˆ‡åˆ† Logits
        all_logps = self.get_batch_logps(
            all_logits,
            concatenated_batch["concatenated_labels"],
            average_log_prob=self.loss_type == "ipo",
            is_encoder_decoder=self.is_encoder_decoder,
            label_pad_token_id=self.label_pad_token_id,
        )

        chosen_logps = all_logps[:len_chosen]
        rejected_logps = all_logps[len_chosen:]
        
        # ==========================================
        # ğŸ”¥ HACK: è¿™é‡Œæˆ‘ä»¬éœ€è¦é‡æ–°è®¡ç®— Rejected çš„ Logps 
        # å› ä¸ºçˆ¶ç±»æ–¹æ³• get_batch_logps æ²¡æ³•ä¼  maskï¼Œæˆ‘ä»¬åœ¨ä¸Šé¢è®¡ç®—äº†ä¸€æ¬¡æ ‡å‡†çš„
        # ç°åœ¨æˆ‘ä»¬è¦æ‰‹åŠ¨é‡ç®—ä¸€æ¬¡ "åŠ æƒç‰ˆ" çš„ rejected_logps
        # ==========================================
        
        # æå– Rejected éƒ¨åˆ†çš„ Logits å’Œ Labels
        rejected_logits = all_logits[len_chosen:]
        rejected_labels = concatenated_batch["concatenated_labels"][len_chosen:]
        
        # æå–æˆ‘ä»¬çš„è‡ªå®šä¹‰ Mask
        extra_mask = batch["extra_rejected_mask"].to(self.accelerator.device)
        
        # è°ƒç”¨æˆ‘ä»¬é­”æ”¹çš„ _get_batch_logps
        focused_rejected_logps, _ = self._get_batch_logps(
            rejected_logits,
            rejected_labels,
            average_log_prob=self.loss_type == "ipo",
            is_encoder_decoder=self.is_encoder_decoder,
            label_pad_token_id=self.label_pad_token_id,
            is_rejected=True,       # <--- å¼€å¯ Focused æ¨¡å¼
            focused_mask=extra_mask # <--- ä¼ å…¥ Mask
        )

        chosen_logits = all_logits[:len_chosen]
        rejected_logits = all_logits[len_chosen:]

        # è¿”å›ä¿®æ”¹åçš„ rejected_logps
        return (chosen_logps, focused_rejected_logps, chosen_logits, rejected_logits)


# =============================================================================
# 3. æ•°æ®é¢„å¤„ç† (è¡Œå· -> Token Mask æ˜ å°„)
# =============================================================================
def preprocess_data(examples, tokenizer, max_length=2048):
    new_examples = {
        "prompt": [],
        "chosen": [],
        "rejected": [],
        "extra_rejected_mask": []
    }
    
    for prompt, chosen, rejected, error_lines in zip(examples['prompt'], examples['chosen'], examples['rejected'], examples['error_lines']):
        
        # 1. Tokenize Rejected (å¸¦ Offset Mapping)
        # æˆ‘ä»¬éœ€è¦æ‰‹åŠ¨æ‹¼æ¥ Prompt + Rejected æ‰èƒ½å¾—åˆ°å®Œæ•´çš„ input_ids
        # DPO Trainer å†…éƒ¨æ˜¯ Prompt + Responseï¼Œæ‰€ä»¥æˆ‘ä»¬è¿™é‡Œæ¨¡æ‹Ÿä¸€ä¸‹
        
        # è¿™é‡Œçš„é€»è¾‘ç¨å¾®å¤æ‚ï¼šDPOTrainer ä¼šå†æ¬¡ Tokenizeã€‚
        # ä¸ºäº†ä¸å‡ºé”™ï¼Œæˆ‘ä»¬å¿…é¡»é¢„å…ˆ Tokenize å¥½ï¼Œç„¶åä»¥ `input_ids` å½¢å¼ä¼ ç»™ Trainerã€‚
        # ä½† trl æ”¯æŒé¢„å¤„ç†å¥½çš„ datasetã€‚
        
        # æ„é€ å®Œæ•´æ–‡æœ¬
        full_rejected_text = prompt + rejected
        
        tokenized_rej = tokenizer(
            full_rejected_text,
            return_offsets_mapping=True,
            add_special_tokens=False, # åé¢ç»Ÿä¸€åŠ 
            truncation=True,
            max_length=max_length
        )
        
        # 2. è®¡ç®— Prompt çš„é•¿åº¦ (Token æ•°)
        tokenized_prompt = tokenizer(
            prompt,
            add_special_tokens=False,
            truncation=True,
            max_length=max_length
        )
        prompt_len = len(tokenized_prompt['input_ids'])
        
        # 3. ç”Ÿæˆ Mask
        # é»˜è®¤æƒé‡ 0.1 (Correct Code Protection)
        # é”™è¯¯è¡Œæƒé‡ 1.0 (Full Penalty)
        if not error_lines:
            mask = [1.0] * len(tokenized_rej['input_ids'])
        else:
            # æœ‰å…·ä½“é”™è¯¯è¡Œï¼Œæ‰§è¡Œ Focused é€»è¾‘
            mask = [0.1] * len(tokenized_rej['input_ids'])
        
        offsets = tokenized_rej['offset_mapping']
        
        # è®¡ç®— Rejected éƒ¨åˆ†æ¯ä¸€è¡Œçš„å­—ç¬¦èŒƒå›´
        # æ³¨æ„ï¼šerror_lines æ˜¯ç›¸å¯¹äº `rejected` å­—ç¬¦ä¸²çš„è¡Œå·
        # Prompt éƒ¨åˆ†æˆ‘ä»¬ä¸å…³å¿ƒï¼Œæˆ‘ä»¬åªçœ‹ Rejected éƒ¨åˆ†çš„ Token
        
        # å…ˆè®¡ç®— Rejected å­—ç¬¦ä¸²å„è¡Œçš„ offset
        rej_lines = rejected.split('\n')
        line_char_ranges = []
        curr = 0
        for line in rej_lines:
            line_char_ranges.append((curr, curr + len(line)))
            curr += len(line) + 1 # +1 for \n
            
        prompt_char_len = len(prompt)
        
        target_lines = set(error_lines)
        
        for i, (start, end) in enumerate(offsets):
            # å¦‚æœè¿™ä¸ª token å±äº promptï¼Œç»™ 0 æƒé‡ (DPO é»˜è®¤ä¹Ÿä¸ç®— prompt lossï¼Œè¿™é‡ŒåŒä¿é™©)
            if i < prompt_len:
                mask[i] = 0.0
                continue
                
            # è¿™ä¸ª token åœ¨ rejected å­—ç¬¦ä¸²ä¸­çš„ç›¸å¯¹ä½ç½®
            rel_start = start - prompt_char_len
            rel_end = end - prompt_char_len
            
            if rel_start < 0: # è¿˜åœ¨ prompt é‡Œ
                mask[i] = 0.0
                continue
                
            token_mid = (rel_start + rel_end) / 2
            
            # åˆ¤æ–­å±äºå“ªä¸€è¡Œ
            for line_idx, (l_start, l_end) in enumerate(line_char_ranges):
                if l_start <= token_mid < l_end:
                    if line_idx in target_lines:
                        mask[i] = 1.0 # å‘½ä¸­é”™è¯¯è¡Œï¼Œå…¨é¢æƒ©ç½š
                    break
        
        new_examples['prompt'].append(prompt)
        new_examples['chosen'].append(chosen)
        new_examples['rejected'].append(rejected)
        new_examples['extra_rejected_mask'].append(mask)

    return new_examples

# =============================================================================
# 4. Main Execution
# =============================================================================
@dataclass
class ScriptArguments:
    model_name_or_path: str = field(metadata={"help": "SFT Model Path"})
    data_path: str = field(metadata={"help": "Data Path (must contain error_lines)"})

def main():
    parser = HfArgumentParser((ScriptArguments, DPOConfig))
    script_args, training_args = parser.parse_args_into_dataclasses()

    # 1. Load Models
    print(f"Loading model from {script_args.model_name_or_path}")
    model = AutoModelForCausalLM.from_pretrained(
        script_args.model_name_or_path,
        trust_remote_code=True,
        use_cache=False
    )
    ref_model = AutoModelForCausalLM.from_pretrained(
        script_args.model_name_or_path,
        trust_remote_code=True,
        use_cache=False
    )
    
    tokenizer = AutoTokenizer.from_pretrained(script_args.model_name_or_path)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # 2. Load & Process Data
    dataset = load_dataset('json', data_files=script_args.data_path, split='train')
    
    print("Preprocessing data for Focused DPO...")
    # ä½¿ç”¨ map è¿›è¡Œé¢„å¤„ç†ï¼Œç”Ÿæˆ extra_rejected_mask
    dataset = dataset.map(
        preprocess_data,
        fn_kwargs={"tokenizer": tokenizer, "max_length": training_args.max_length},
        batched=True,
        batch_size=1000,
        remove_columns=dataset.column_names # ç§»é™¤æ—§åˆ—ï¼Œæ¢æˆæ–°ç”Ÿæˆçš„
    )
    
    # 3. Trainer
    trainer = FocusedDPOTrainer(
        model=model,
        ref_model=ref_model,
        args=training_args,
        train_dataset=dataset,
        tokenizer=tokenizer,
        data_collator=FocusedDPODataCollatorWithPadding(tokenizer), # ä½¿ç”¨è‡ªå®šä¹‰ collator
    )

    print("Starting Focused-DPO Training...")
    trainer.train()
    trainer.save_model(training_args.output_dir)
    tokenizer.save_pretrained(training_args.output_dir)

if __name__ == "__main__":
    main()