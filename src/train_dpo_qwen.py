import os
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from transformers import AutoModelForCausalLM, AutoTokenizer, get_scheduler
import deepspeed
from datasets import load_dataset
import argparse
from tqdm import tqdm

# ================= DPO Loss å®ç° =================
def dpo_loss(policy_chosen_logps, policy_rejected_logps, ref_chosen_logps, ref_rejected_logps, beta=0.1):
    """åŸç”Ÿ PyTorch å®ç°çš„ DPO Loss"""
    pi_logratios = policy_chosen_logps - policy_rejected_logps
    ref_logratios = ref_chosen_logps - ref_rejected_logps
    logits = pi_logratios - ref_logratios
    losses = -F.logsigmoid(beta * logits)
    rewards = beta * (pi_logratios - ref_logratios).detach()
    return losses.mean(), rewards.mean()

def get_batch_logps(logits, labels, average_log_prob=False):
    """è®¡ç®— Log Probabilities"""
    # Shift so that tokens < n predict n
    shift_logits = logits[..., :-1, :].contiguous()
    shift_labels = labels[..., 1:].contiguous()
    
    loss_fct = torch.nn.CrossEntropyLoss(reduction='none')
    shift_logits = shift_logits.view(-1, shift_logits.size(-1))
    shift_labels = shift_labels.view(-1)
    
    # å¿½ç•¥ padding (-100)
    token_logps = -loss_fct(shift_logits, shift_labels)
    token_logps = token_logps.view(labels.shape[0], -1)
    
    # Sum over sequence
    # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ padding çš„ loss å·²ç»æ˜¯ 0ï¼ˆCrossEntropyLoss é»˜è®¤è¡Œä¸ºï¼‰
    if average_log_prob:
        return token_logps.sum(-1) / (shift_labels != -100).sum(-1)
    else:
        return token_logps.sum(-1)

# ================= æ•°æ®é›†ç±» =================
class QwenChatMLDataset(Dataset):
    def __init__(self, data_path, tokenizer, max_length=2048):
        self.data = load_dataset("json", data_files=data_path, split="train")
        self.tokenizer = tokenizer
        self.max_length = max_length
        # é¢„å…ˆæ„å»º ChatML æ¨¡æ¿éƒ¨åˆ†
        self.system = "<|im_start|>system\nYou are a helpful and efficient AI programming assistant.<|im_end|>\n"
        self.user_start = "<|im_start|>user\n"
        self.user_end = "<|im_end|>\n"
        self.assist_start = "<|im_start|>assistant\n"
        self.assist_end = "<|im_end|>\n"

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        
        # æ‰‹åŠ¨æ„å»º Prompt (é¿å…ä¾èµ– tokenizer.apply_chat_template å¯¼è‡´çš„ä¸ç¡®å®šæ€§)
        prompt_str = f"{self.system}{self.user_start}{item['prompt']}{self.user_end}{self.assist_start}"
        
        def tokenize_pair(p_str, answer_str):
            full_text = p_str + answer_str + self.assist_end
            
            # è¿™é‡Œçš„ padding=Falseï¼Œæˆ‘ä»¬åœ¨ collate_fn é‡Œåš padding
            enc = self.tokenizer(
                full_text, 
                max_length=self.max_length, 
                truncation=True, 
                add_special_tokens=False
            )
            input_ids = enc['input_ids']
            attention_mask = enc['attention_mask']
            
            # æ„å»º Labelsï¼šPrompt éƒ¨åˆ†è®¾ä¸º -100
            prompt_enc = self.tokenizer(p_str, add_special_tokens=False)['input_ids']
            labels = list(input_ids)
            if len(prompt_enc) < len(labels):
                for i in range(len(prompt_enc)):
                    labels[i] = -100
            else:
                # æç«¯æƒ…å†µï¼šPrompt è¢«æˆªæ–­
                return None
                
            return {
                "input_ids": torch.tensor(input_ids, dtype=torch.long),
                "attention_mask": torch.tensor(attention_mask, dtype=torch.long),
                "labels": torch.tensor(labels, dtype=torch.long)
            }

        chosen = tokenize_pair(prompt_str, item['chosen'])
        rejected = tokenize_pair(prompt_str, item['rejected'])
        
        if chosen is None or rejected is None:
            return None
            
        return {
            "chosen_input_ids": chosen["input_ids"],
            "chosen_attention_mask": chosen["attention_mask"],
            "chosen_labels": chosen["labels"],
            "rejected_input_ids": rejected["input_ids"],
            "rejected_attention_mask": rejected["attention_mask"],
            "rejected_labels": rejected["labels"],
        }

def collate_fn(batch):
    # è¿‡æ»¤ None
    batch = [x for x in batch if x is not None]
    if len(batch) == 0: return None
    
    pad_id = 151643 # Qwen pad_token_id (æˆ–é€šè¿‡ tokenizer è·å–)
    
    out = {}
    for key in batch[0].keys():
        # labels ç”¨ -100 å¡«å……ï¼Œå…¶ä»–ç”¨ pad_id å¡«å……
        padding_value = -100 if "labels" in key else pad_id
        # attention_mask ç”¨ 0 å¡«å……
        if "attention_mask" in key: padding_value = 0
            
        tensors = [x[key] for x in batch]
        out[key] = torch.nn.utils.rnn.pad_sequence(tensors, batch_first=True, padding_value=padding_value)
    return out

# ================= ä¸»ç¨‹åº =================
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_name_or_path", type=str, required=True)
    parser.add_argument("--data_path", type=str, required=True)
    parser.add_argument("--output_dir", type=str, required=True)
    parser.add_argument("--local_rank", type=int, default=-1)
    parser.add_argument("--max_length", type=int, default=2048)
    parser.add_argument("--beta", type=float, default=0.1)
    parser.add_argument("--learning_rate", type=float, default=5e-7)
    parser.add_argument("--num_epochs", type=int, default=1)
    
    # ================= ğŸ”§ å…³é”®ä¿®æ”¹ï¼šæ·»åŠ  DeepSpeed å¿…é¡»çš„å‚æ•° =================
    # DeepSpeed éœ€è¦è¿™äº›æ¥è®¡ç®— "auto" çš„é…ç½®
    parser.add_argument("--per_device_train_batch_size", type=int, default=1)
    parser.add_argument("--gradient_accumulation_steps", type=int, default=8)
    # ======================================================================

    parser = deepspeed.add_config_arguments(parser)
    args = parser.parse_args()

    # åˆå§‹åŒ– Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
    
    # 1. åˆå§‹åŒ– Dataset å’Œ DataLoader
    dataset = QwenChatMLDataset(args.data_path, tokenizer, args.max_length)
    
    # ä½¿ç”¨ args é‡Œçš„ batch size
    train_dataloader = DataLoader(
        dataset, 
        batch_size=args.per_device_train_batch_size, # è¿™é‡Œä½¿ç”¨ä¼ å…¥çš„å‚æ•°
        shuffle=True, 
        collate_fn=collate_fn, 
        num_workers=0
    )

    # 2. åŠ è½½æ¨¡å‹ (Policy)
    policy_model = AutoModelForCausalLM.from_pretrained(
        args.model_name_or_path, 
        trust_remote_code=True, 
        torch_dtype=torch.bfloat16
    )
    policy_model.gradient_checkpointing_enable()

    # 3. åŠ è½½å‚è€ƒæ¨¡å‹ (Ref)
    ref_model = AutoModelForCausalLM.from_pretrained(
        args.model_name_or_path, 
        trust_remote_code=True, 
        torch_dtype=torch.bfloat16
    )
    ref_model.eval()

    # 4. åˆå§‹åŒ– DeepSpeed
    # åªéœ€è¦æŠŠ Policy Model ä¼ ç»™ DeepSpeed è¿›è¡Œä¼˜åŒ–
    model_engine, optimizer, _, _ = deepspeed.initialize(
        args=args,
        model=policy_model,
        model_parameters=policy_model.parameters()
    )
    
    # Reference Model ä¹Ÿéœ€è¦æ”¾åˆ°æ­£ç¡®çš„è®¾å¤‡ä¸Šï¼Œä½†ä¸éœ€è¦ DeepSpeed ä¼˜åŒ–å™¨
    # ç®€å•åšæ³•ï¼šæˆ‘ä»¬è®© DeepSpeed ä¹Ÿç®¡ç†å®ƒï¼ˆä½œä¸º Inference Engineï¼‰æˆ–è€…æ‰‹åŠ¨æ”¾
    # ä¸ºäº†å…¼å®¹ ZeRO-3ï¼Œæˆ‘ä»¬æœ€å¥½ä¹Ÿç”¨ deepspeed.init_inference æˆ–è€…ç®€å•çš„ .to(device)
    # æ³¨æ„ï¼šZeRO-3 ä¸‹ ref_model æ˜¾å­˜å ç”¨æ˜¯ä¸ªé—®é¢˜ã€‚è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå‡è®¾æ˜¾å­˜è¶³å¤Ÿæˆ–ä¾é  offloadã€‚
    ref_engine, _, _, _ = deepspeed.initialize(
        args=args,
        model=ref_model,
        optimizer=None # Ref model ä¸ä¼˜åŒ–
    )

    # è®­ç»ƒå¾ªç¯
    global_step = 0
    for epoch in range(args.num_epochs):
        model_engine.train()
        progress_bar = tqdm(train_dataloader, desc=f"Epoch {epoch}", disable=(args.local_rank != 0))
        
        for batch in progress_bar:
            if batch is None: continue
            
            # Move batch to device
            device = model_engine.device
            batch = {k: v.to(device) for k, v in batch.items()}
            
            # --- Forward Pass (Policy) ---
            chosen_logps = get_batch_logps(
                model_engine(input_ids=batch['chosen_input_ids'], attention_mask=batch['chosen_attention_mask']).logits,
                batch['chosen_labels']
            )
            rejected_logps = get_batch_logps(
                model_engine(input_ids=batch['rejected_input_ids'], attention_mask=batch['rejected_attention_mask']).logits,
                batch['rejected_labels']
            )
            
            # --- Forward Pass (Reference) ---
            with torch.no_grad():
                ref_chosen_logps = get_batch_logps(
                    ref_engine(input_ids=batch['chosen_input_ids'], attention_mask=batch['chosen_attention_mask']).logits,
                    batch['chosen_labels']
                )
                ref_rejected_logps = get_batch_logps(
                    ref_engine(input_ids=batch['rejected_input_ids'], attention_mask=batch['rejected_attention_mask']).logits,
                    batch['rejected_labels']
                )

            # --- Loss Calculation ---
            loss, reward = dpo_loss(
                chosen_logps, rejected_logps, 
                ref_chosen_logps, ref_rejected_logps, 
                beta=args.beta
            )
            
            # --- Backward ---
            model_engine.backward(loss)
            model_engine.step()
            
            global_step += 1
            if args.local_rank == 0 and global_step % 5 == 0:
                progress_bar.set_postfix(loss=loss.item(), reward=reward.item())
                
            if global_step % 100 == 0 and args.local_rank == 0:
                print(f"Step {global_step} | Loss: {loss.item():.4f} | Reward: {reward.item():.4f}")

        # ä¿å­˜
        if args.local_rank == 0:
            print(f"Saving epoch {epoch}...")
            # æ³¨æ„ï¼šZeRO-3 ä¿å­˜éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œè¿™é‡Œç®€å•ç¤ºæ„ï¼Œå»ºè®®ä½¿ç”¨ model_engine.save_checkpoint
            # ä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬åªä¿å­˜ tokenizer
            tokenizer.save_pretrained(args.output_dir)

    # æœ€ç»ˆä¿å­˜
    model_engine.save_checkpoint(args.output_dir)

if __name__ == "__main__":
    main()